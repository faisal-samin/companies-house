{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enriching Companies House data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/ch_screenshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives an overview on how business data from Companies House is imported, formatting and then enriched by exploiting various APIs and websites. Examples of applications include:\n",
    "-  Retrieving official company websites using Google Places API\n",
    "-  Scraping websites to get keywords to classify the industry of businesses.\n",
    "-  Obtaining social media accounts and handles for companies and then use these to get a proxy for their web presence (number of followers, likes etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Free Company Data Product is a downloadable data snapshot containing basic company data of live companies on the Companies House register, and is the principal dataset for this project. This is updated monthly and needs to be downloaded before importing as a pandas dataframe. First, we need to import some modules..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Pandas: provide easy-to-use data structures in Python\n",
    "-  Numpy: provides fast and efficient multidimensional arrays, in addition to linear algebra and mathematical operations.\n",
    "-  Matplotlib: provides plot to visualise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Increase figure and font sizes for easier viewing\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and formatting Companies House dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latest version of the Free Company Data Product can be downloaded here. http://download.companieshouse.gov.uk/en_output.html. The zip file that is downloaded is approximately 300MB, and the raw CSV file around 2GB. Once downloaded, ensure that the data is saved in the root folder of this notebook (or amend directory as required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do: investigate warning on mixed data types\n",
    "ch_raw = pd.read_csv('/Users/dataexploitationmac1/Desktop/Faisal/Datasets/BasicCompanyDataAsOneFile-2018-02-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preview the data\n",
    "ch_raw.head(10) # first 10 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fields available\n",
    "ch_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary columns for this project\n",
    "# why is copy() used? See explanation at link below:\n",
    "# https://stackoverflow.com/questions/27673231/why-should-i-make-a-copy-of-a-data-frame-in-pandas\n",
    "ch = ch_raw.iloc[:,[0,1,4,5,6,7,8,9,10,11,12,18,19,21,26,27,28,29]].copy()\n",
    "\n",
    "# rename columns\n",
    "ch.columns = ['name','crn','address1','address2','postTown','county','country', \\\n",
    "            'postcode','category','status','origin','accounts_lastMadeUpDate','accountCategory',\\\n",
    "            'returns_lastMadeUpDate','sic1','sic2','sic3','sic4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format missing values\n",
    "ch.sic1.replace('None Supplied', np.NaN, inplace=True)\n",
    "ch = ch.dropna(subset=['name']) # delete rows with null business names (usually only a few values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Produce a range of key stats \n",
    "# Key stats\n",
    "print('---------')\n",
    "print('Number of businesses: %s' %len(ch))\n",
    "print('Missing SIC codes: %s' %ch.sic1.isnull().sum())\n",
    "sic_comp = (1.0 - (float(ch.sic1.isnull().sum())/len(ch)))*100\n",
    "print('SIC code completion: %.2f' %sic_comp + '%')\n",
    "post_comp = (1.0 - (float(ch.postcode.isnull().sum())/len(ch)))*100\n",
    "print('Postcode completion: %.2f' %post_comp + '%')\n",
    "print('---------')\n",
    "print('Category breakdown (top 5)')\n",
    "print('')\n",
    "print(ch.category.value_counts().head())\n",
    "print('---------')\n",
    "print('Account category (top 5)')\n",
    "print('')\n",
    "print(ch.accountCategory.value_counts().head())\n",
    "print('---------')\n",
    "print('Geographical breakdown (top 5)')\n",
    "print('')\n",
    "print(ch.origin.value_counts().head())\n",
    "print('---------')\n",
    "print('SIC code breakdown (top 5)')\n",
    "print('')\n",
    "print(ch.sic1.value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some pandas commands to explore the dataset, including setting up a function to find companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.dtypes # types of each column - all objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_company(name):\n",
    "    '''\n",
    "    Searches companies house dataset for company name which include the given input which must be a string.\n",
    "    '''\n",
    "    name = name.lower()\n",
    "    n = ch.name.str.lower().str.contains(name)\n",
    "    x = input(str(n.sum()) + ' companies found. See list of companies? Y or N? ')\n",
    "    if x.lower() == 'y':\n",
    "        return ch[n]\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_company('Burberry') # testing function on a few cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_company('Dyson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring SIC codes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.sic1.describe() # counts occurences and unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting by the top 20 SIC codes shows that some of these are not very descriptive. Top of the list is 'Other business support service activities n.e.c'. Third is 'Dormant Company' and this is followed by 'Other service activities n.e.c'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ch.sic1.value_counts().head(20) # sort by top 20 sic codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.sic1.value_counts().head(30).plot() # shows skew of top categories\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if company reference numbers are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.crn.describe() # all crns are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.crn.isnull().sum() # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the address data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.head() # reminder of the address fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.address1.describe() # 1.6 million unique address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.address1.isnull().sum() # 27K null addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.postTown.isnull().sum() # 93K missing town names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.postcode.isnull().sum() # 52K missing post codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export formatted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataset, named after MMYY of ch data\n",
    "ch.to_csv('ch_2018-02.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: remove non-UK companies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_uk = ch[ch['origin'].isin(['United Kingdom','Great Britain','UNITED KINGDOM','GREAT BRITAIN','ENGLAND & WALES','UK'])]\n",
    "ch_uk.reset_index(inplace=True)\n",
    "ch_uk.to_csv('ch_2018-02.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping data from Google Search Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section goes through the process of running google searches of business names in Companies House, scraping text from the results, and then returning a wordcloud of text from the first page of results.\n",
    "\n",
    "The code below builds up the code for functions that run searches and produce\n",
    "worldclouds as follows:\n",
    "\n",
    "cloud(keyWords(search('Company Name')))\n",
    "\n",
    "- search(string): returns a list of URLs from Google for the given term\n",
    "- keyWords(list): screen-scrapes all visible text from the given list of URLs, and cleans\n",
    "- cloud(string): after removing a given list of stopwords, produces a wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing further modules and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser # to open web links\n",
    "import nltk # natural language toolkit\n",
    "from nltk.corpus import stopwords # Import the stop word list, may require download\n",
    "# WordCloud modules\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "import re # regular expressions \n",
    "from time import sleep # to pause web-scraper\n",
    "import requests # allows you to send HTTP requests via Python\n",
    "from bs4 import BeautifulSoup # beautiful soup for parsing of HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in formatted CH dataset if starting a new session. We'll refer to this at the end of the section after we've built up our tools to scrape and clean website text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = pd.read_csv('ch_2018-02.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returning links for a Google Search term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to build some functionality to scrape the search results returned by Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set business search term as an example\n",
    "biz = 'DYSON LIMITED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read HTML\n",
    "html = requests.get('https://www.google.co.uk/search?q='+ biz)\n",
    "# Parse HTML into a BeautifulSoup object\n",
    "soup = BeautifulSoup(html.content, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all links and put into list\n",
    "list_of_links = []\n",
    "for link in soup.find_all('a'):\n",
    "    list_of_links.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(list_of_links) # needs cleaning up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up results\n",
    "links = DataFrame({'urls':list_of_links}) #turn list into DF\n",
    "links = links[links.urls.str.contains('/url?')] #Only search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove cached sites\n",
    "links = links[links.urls.str.contains('webcache.googleusercontent') == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove opening url?q= string\n",
    "links = links.urls.str.replace('/url\\?q=',\"\")\n",
    "# after this, you don't need to call list anymore on the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove suffixed &sa bit by splitting and drop index\n",
    "links = links.str.split('&sa',1).reset_index().drop('index',1)\n",
    "# this is now a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use iterrows to grab first entry in each list which should be the working url\n",
    "links_cleaned = []\n",
    "for row in links.iterrows():\n",
    "    links_cleaned.append(row[1][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe\n",
    "links_cleaned = DataFrame(links_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now bring this together in one function. Note that we're screen-scraping from Google Search results so we'll need to be careful to not overload Google with search requests in quick succession (and potentially get our IP blocked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def search_google(business_name):\n",
    "    '''\n",
    "    Takes in a business name and returns the links returned in the first page of Google Search results\n",
    "    '''\n",
    "    # Read HTML\n",
    "    html = requests.get('https://www.google.co.uk/search?q='+business_name)\n",
    "    # Parse HTML into a BeautifulSoup object\n",
    "    soup = BeautifulSoup(html.content, 'html5lib')\n",
    "\n",
    "    # Get all links and put into list\n",
    "    list_of_links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        list_of_links.append(link.get('href'))\n",
    "\n",
    "    # Cleaning up results\n",
    "    links = DataFrame({'urls':list_of_links}) #turn list into DF\n",
    "    links = links[links.urls.str.contains('/url?')] #Only search results\n",
    "    \n",
    "    # remove cached sites\n",
    "    links = links[links.urls.str.contains('webcache.googleusercontent') == False]\n",
    "\n",
    "    # remove opening url?q= string\n",
    "    links = links.urls.str.replace('/url\\?q=',\"\")\n",
    "    # after this, you don't need to call list anymore on the column\n",
    "\n",
    "    # remove suffixed &sa bit by splitting and drop index\n",
    "    links = links.str.split('&sa',1).reset_index().drop('index',1)\n",
    "    # this is now a dataframe\n",
    "\n",
    "    # use iterrows to grab first entry in each list which should be the working url\n",
    "    links_cleaned = []\n",
    "    for row in links.iterrows():\n",
    "        links_cleaned.append(row[1][0][0])\n",
    "        \n",
    "    # convert to dataframe\n",
    "    links_cleaned = DataFrame(links_cleaned)\n",
    "\n",
    "    return links_cleaned[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "burberry_links = search_google('burberry limited')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use webbrowser library to open all links in browser (if needed)\n",
    "for link in burberry_links:\n",
    "    webbrowser.open(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract key text from company websites "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got the functionality to return links from Google search results, we want to navigate to each link, scrape and format the text to find words with explanatory value after removing stopwords and other standard website text. Let's use the Dyson website as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyson_links = search_google('Dyson Limited')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the first search result\n",
    "# Read HTML\n",
    "html = requests.get(dyson_links[0])\n",
    "# Parse HTML into a BeautifulSoup object\n",
    "soup = BeautifulSoup(html.content, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting text from key html sections\n",
    "[s.extract() for s in soup(['style', 'script', '[document]', 'head', 'title'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all visible text\n",
    "text = soup.getText().encode('ascii','ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to clean up this text by removing HTML tags and new line indicators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_r = str(text).replace('\\\\n','').replace('\\\\t','').replace('\\\\r','')\n",
    "print(text_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def stripsymbols(text):\n",
    "    '''\n",
    "    Use regular expressions to do a find-and-replace of HTML text.\n",
    "    Function found online\n",
    "    '''\n",
    "    text = str(text)\n",
    "    x = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text)\n",
    "    x = re.sub('/(^|\\b)@\\S*($|\\b)/',\" \",x)\n",
    "    x = re.sub('/(^|\\b)#\\S*($|\\b)/',\" \",x)\n",
    "    x = re.sub(\"[^a-zA-Z]\",\" \",x)\n",
    "    x = re.sub(r\"(?:\\@|https?\\://)\\S+\", \" \",x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_stripped = stripsymbols(text_r)\n",
    "print(text_stripped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting better! The final step is remove stop words by invoking the Natural Language Toolkit Library we imported earlier. Let's see what kind of stop words are classified in the library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words('English'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a function to remove stopwords from website text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(text):\n",
    "    '''\n",
    "    Remove stopwords from given text string\n",
    "    '''\n",
    "    words = [w for w in text if not w in stopwords.words(\"english\")]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text for function by (a) changing to lowercase (2) remove whitespace from beginning and end \n",
    "# (3) splitting the text to create a list of individual wordsvisible_text_stripped.lower().strip().split() \n",
    "text_stripped_split = text_stripped.lower().strip().split()\n",
    "text_stripped_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, running this through the removeStopWords function gives us something in a much better shape than the original text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaned = removeStopWords(text_stripped_split)\n",
    "text_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapping this all up into a function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleantext(text):\n",
    "    text_r = str(text).replace('\\\\n','').replace('\\\\t','').replace('\\\\r','')\n",
    "    text_stripped = stripsymbols(text_r)\n",
    "    text_stripped_split = text_stripped.lower().strip().split()\n",
    "    text_cleaned = removeStopWords(text_stripped_split)\n",
    "    return ' '.join(text_cleaned) # returns a joined list of the remaining words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyson_cleaned = cleantext(text) # example of a function\n",
    "dyson_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Natural Language Toolkit to tokenise and tag words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another feature of the Natural Language Toolkit we can exploit is the ability to tag words and categorise them into their 'parts of speech' (i.e. nouns, verbs, adverbs). Perhaps this can be used to tag words from the scraped text, and retrieve nouns under the assumption that they provide the greatest explanatory power.\n",
    "\n",
    "Let's test this with the scraped text from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyson_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(dyson_cleaned) # tokenize (split) the string\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tags bellow are given according to the Penn Treebank Project here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "The results are quite interesting. Words like 'vacuums', 'cylinders', 'robot' and 'dryers' are correctly given as nouns. However, some words are double-tagged. For instance, 'dyson' is given as a verb as well as a noun. 'Airblade' is given as a past-tense verb. \n",
    "\n",
    "From this example alone, filtering on nouns would yield the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(tokens) # categorise words according to their parts of speech\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep nouns only\n",
    "tagged_nouns = \" \".join([word[0] for word in tagged if word[1] in ['NNS','NN']])\n",
    "tagged_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference in list sizes - 1,352 removed words (out of 3,767)\n",
    "len(dyson_cleaned) - len(tagged_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Master function and wordclouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect everything we've built in Section 2 and build our final master functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_words(list_of_urls):\n",
    "    '''\n",
    "    From the given list of urls, this function scrapes, cleans (removing symbols and non-nouns)\n",
    "    and gathers all text into a single string\n",
    "    '''\n",
    "    \n",
    "    words_cleaned = '' # Set up empty string\n",
    "    \n",
    "    for website in list_of_urls:\n",
    "        # try / except for troublesome websites\n",
    "        try:\n",
    "            html = requests.get(website)\n",
    "            soup = BeautifulSoup(html.content, 'html5lib')\n",
    "            [s.extract() for s in soup(['style', 'script', '[document]', 'head', 'title'])]\n",
    "            visibleText = soup.getText().encode('ascii','ignore')\n",
    "            words_cleaned += cleantext(visibleText) # Calling the function we built earlier \n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    # Tokenize and tag words\n",
    "    tokens = nltk.word_tokenize(words_cleaned)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Keep nouns only\n",
    "    keywords = \" \".join([word[0] for word in tagged if word[1] in ['NNS','NN']])\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words(search_google('Dyson Limited')) # testing on an example - takes a few seconds to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is best visualised in a word cloud..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cloud(words):\n",
    "    '''\n",
    "    Function that takes in a string and produces a word cloud\n",
    "    '''\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                          background_color='black',\n",
    "                          width=1800,\n",
    "                          height=1400\n",
    "                         ).generate(words)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud(key_words(search_google('Royal Dutch Shell'))) # can take upto a minute to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud(key_words(search_google('BT Group PLC')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of scope to improve this. Certain terms from websites that commonly re-occur can be filtered out by updating the stopwords set as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ways to improve this: list of common names, weights and quantities, months and dates,\n",
    "\n",
    "custom_words = ['sun','new','showbiz','tv','uk','john','lewis','partnership','offers','store',\n",
    "'business','company','stores','shop','department','partner','street','london','partners','peter',\n",
    "'jones','duration', 'views', 'minutes', 'month', 'version', 'system','tesco','september','privacy',\n",
    "'policy','customer','service','home', 'company', 'london', 'price', 'offer', 'customer',\n",
    "'service', 'home', 'year', 'london', 'day', 'march', 'business', 'shop','item','level','logo','menu',\n",
    "'account','co','road','centre']\n",
    "\n",
    "\n",
    "#STOPWORDS is a set, so need to use update method\n",
    "STOPWORDS.update(custom_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Google Places API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've relied on Google Search results to hopefully get websites that relate to a company term. However, the Google Places API can be linked to with a company's name and postcode and used to retrieve the company website and other details. \n",
    "\n",
    "Information from Google Places is displayed for certain businesses and places of interest as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/google_places_screenshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin as usual by importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lib import PostCodeClient # pip install postcode.io\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Places can be called with the python-google-places package: https://github.com/slimkrazy/python-google-places. To use this, you'll need to set up a Google developer account here http://code.google.com/apis/console. By default, you'll be able to send 1,000 requests per day but this can be increased free of charge, up to 150,000 requests per 24 hour period, by enabling billing on the Google API Console to verify your identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleplaces import GooglePlaces, types, lang\n",
    "YOUR_API_KEY = '' # enter API key which you'll receive from Google \n",
    "google_places = GooglePlaces(YOUR_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Check documentation here: https://developers.google.com/places/web-service/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "830px",
    "left": "1185px",
    "top": "127px",
    "width": "233px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
