{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enriching Companies House data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![title](Images/ch_screenshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives an overview on how business data from Companies House is imported, formatting and then enriched by exploiting various APIs and websites. Examples of applications include:\n",
    "-  Retrieving official company websites using Google Places API\n",
    "-  Scraping websites to get keywords to classify the industry of businesses.\n",
    "-  Obtaining social media accounts and handles for companies and then use these to get a proxy for their web presence (number of followers, likes etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Free Company Data Product is a downloadable data snapshot containing basic company data of live companies on the Companies House register, and is the principal dataset for this project. This is updated monthly and needs to be downloaded before importing as a pandas dataframe. First, we need to import some modules..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Pandas: provide easy-to-use data structures in Python\n",
    "-  Numpy: provides fast and efficient multidimensional arrays, in addition to linear algebra and mathematical operations.\n",
    "-  Matplotlib: provides plot to visualise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Increase figure and font sizes for easier viewing\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and formatting Companies House dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The latest version of the Free Company Data Product can be downloaded here. http://download.companieshouse.gov.uk/en_output.html. The zip file that is downloaded is approximately 300MB, and the raw CSV file around 2GB. Once downloaded, ensure that the data is saved in the root folder of this notebook (or amend directory as required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to-do: investigate warning on mixed data types\n",
    "ch_raw = pd.read_csv('/Users/dataexploitationmac1/Desktop/Faisal/Datasets/BasicCompanyDataAsOneFile-2018-02-01.csv')\n",
    "# ch_raw = pd.read_csv('/Users/fasamin/Desktop/DS/Datasets/BasicCompanyDataAsOneFile-2018-02-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preview the data\n",
    "ch_raw.head(10) # first 10 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fields available\n",
    "ch_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove unnecessary columns for this project\n",
    "# why is copy() used? See explanation at link below:\n",
    "# https://stackoverflow.com/questions/27673231/why-should-i-make-a-copy-of-a-data-frame-in-pandas\n",
    "ch = ch_raw.iloc[:,[0,1,4,5,6,7,8,9,10,11,12,18,19,21,26,27,28,29]].copy()\n",
    "\n",
    "# rename columns\n",
    "ch.columns = ['name','crn','address1','address2','postTown','county','country', \\\n",
    "            'postcode','category','status','origin','accounts_lastMadeUpDate','accountCategory',\\\n",
    "            'returns_lastMadeUpDate','sic1','sic2','sic3','sic4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# format missing values\n",
    "ch.sic1.replace('None Supplied', np.NaN, inplace=True)\n",
    "ch = ch.dropna(subset=['name']) # delete rows with null business names (usually only a few values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Produce a range of key stats \n",
    "# Key stats\n",
    "print('---------')\n",
    "print('Number of businesses: %s' %len(ch))\n",
    "print('Missing SIC codes: %s' %ch.sic1.isnull().sum())\n",
    "sic_comp = (1.0 - (float(ch.sic1.isnull().sum())/len(ch)))*100\n",
    "print('SIC code completion: %.2f' %sic_comp + '%')\n",
    "post_comp = (1.0 - (float(ch.postcode.isnull().sum())/len(ch)))*100\n",
    "print('Postcode completion: %.2f' %post_comp + '%')\n",
    "print('---------')\n",
    "print('Category breakdown (top 5)')\n",
    "print('')\n",
    "print(ch.category.value_counts().head())\n",
    "print('---------')\n",
    "print('Account category (top 5)')\n",
    "print('')\n",
    "print(ch.accountCategory.value_counts().head())\n",
    "print('---------')\n",
    "print('Geographical breakdown (top 5)')\n",
    "print('')\n",
    "print(ch.origin.value_counts().head())\n",
    "print('---------')\n",
    "print('SIC code breakdown (top 5)')\n",
    "print('')\n",
    "print(ch.sic1.value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some pandas commands to explore the dataset, including setting up a function to find companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch.dtypes # types of each column - all objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_company(name):\n",
    "    '''\n",
    "    Searches companies house dataset for company name which include the given input which must be a string.\n",
    "    '''\n",
    "    name = name.lower()\n",
    "    n = ch.name.str.lower().str.contains(name)\n",
    "    x = input(str(n.sum()) + ' companies found. See list of companies? Y or N? ')\n",
    "    if x.lower() == 'y':\n",
    "        return ch[n]\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "find_company('Burberry') # testing function on a few cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "find_company('Dyson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring SIC codes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch.sic1.describe() # counts occurences and unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting by the top 20 SIC codes shows that some of these are not very descriptive. Top of the list is 'Other business support service activities n.e.c'. Third is 'Dormant Company' and this is followed by 'Other service activities n.e.c'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ch.sic1.value_counts().head(20) # sort by top 20 sic codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch.sic1.value_counts().head(30).plot() # shows skew of top categories\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if company reference numbers are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch.crn.describe() # all crns are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch.crn.isnull().sum() # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the address data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch.head() # reminder of the address fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch.address1.describe() # 1.6 million unique address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch.address1.isnull().sum() # 27K null addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch.postTown.isnull().sum() # 93K missing town names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch.postcode.isnull().sum() # 52K missing post codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export formatted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export dataset, named after MMYY of ch data\n",
    "ch.to_csv('ch_2018-02.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: remove non-UK companies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_uk = ch[ch['origin'].isin(['United Kingdom','Great Britain','UNITED KINGDOM','GREAT BRITAIN','ENGLAND & WALES','UK'])]\n",
    "ch_uk.reset_index(inplace=True)\n",
    "ch_uk.to_csv('ch_2018-02_uk.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping data from Google Search Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section goes through the process of running google searches of business names in Companies House, scraping text from the results, and then returning a wordcloud of text from the first page of results.\n",
    "\n",
    "The code below builds up the code for functions that run searches and produce\n",
    "worldclouds as follows:\n",
    "\n",
    "cloud(keyWords(search('Company Name')))\n",
    "\n",
    "- search(string): returns a list of URLs from Google for the given term\n",
    "- keyWords(list): screen-scrapes all visible text from the given list of URLs, and cleans\n",
    "- cloud(string): after removing a given list of stopwords, produces a wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing further modules and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import webbrowser # to open web links\n",
    "import nltk # natural language toolkit\n",
    "from nltk.corpus import stopwords # Import the stop word list, may require download\n",
    "# WordCloud modules\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "import re # regular expressions \n",
    "from time import sleep # to pause web-scraper\n",
    "import requests # allows you to send HTTP requests via Python\n",
    "from bs4 import BeautifulSoup # beautiful soup for parsing of HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in formatted CH dataset if starting a new session. We'll refer to this at the end of the section after we've built up our tools to scrape and clean website text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch = pd.read_csv('ch_2018-02.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returning links for a Google Search term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to build some functionality to scrape the search results returned by Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set business search term as an example\n",
    "biz = 'DYSON LIMITED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read HTML\n",
    "html = requests.get('https://www.google.co.uk/search?q='+ biz)\n",
    "# Parse HTML into a BeautifulSoup object\n",
    "soup = BeautifulSoup(html.content, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all links and put into list\n",
    "list_of_links = []\n",
    "for link in soup.find_all('a'):\n",
    "    list_of_links.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(list_of_links) # needs cleaning up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleaning up results\n",
    "links = DataFrame({'urls':list_of_links}) #turn list into DF\n",
    "links = links[links.urls.str.contains('/url?')] #Only search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove cached sites\n",
    "links = links[links.urls.str.contains('webcache.googleusercontent') == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove opening url?q= string\n",
    "links = links.urls.str.replace('/url\\?q=',\"\")\n",
    "# after this, you don't need to call list anymore on the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove suffixed &sa bit by splitting and drop index\n",
    "links = links.str.split('&sa',1).reset_index().drop('index',1)\n",
    "# this is now a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use iterrows to grab first entry in each list which should be the working url\n",
    "links_cleaned = []\n",
    "for row in links.iterrows():\n",
    "    links_cleaned.append(row[1][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to dataframe\n",
    "links_cleaned = DataFrame(links_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now bring this together in one function. Note that we're screen-scraping from Google Search results so we'll need to be careful to not overload Google with search requests in quick succession (and potentially get our IP blocked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_google(business_name):\n",
    "    '''\n",
    "    Takes in a business name and returns the links returned in the first page of Google Search results\n",
    "    '''\n",
    "    # Read HTML\n",
    "    html = requests.get('https://www.google.co.uk/search?q='+business_name)\n",
    "    # Parse HTML into a BeautifulSoup object\n",
    "    soup = BeautifulSoup(html.content, 'html5lib')\n",
    "\n",
    "    # Get all links and put into list\n",
    "    list_of_links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        list_of_links.append(link.get('href'))\n",
    "\n",
    "    # Cleaning up results\n",
    "    links = DataFrame({'urls':list_of_links}) #turn list into DF\n",
    "    links = links[links.urls.str.contains('/url?')] #Only search results\n",
    "    \n",
    "    # remove cached sites\n",
    "    links = links[links.urls.str.contains('webcache.googleusercontent') == False]\n",
    "\n",
    "    # remove opening url?q= string\n",
    "    links = links.urls.str.replace('/url\\?q=',\"\")\n",
    "    # after this, you don't need to call list anymore on the column\n",
    "\n",
    "    # remove suffixed &sa bit by splitting and drop index\n",
    "    links = links.str.split('&sa',1).reset_index().drop('index',1)\n",
    "    # this is now a dataframe\n",
    "\n",
    "    # use iterrows to grab first entry in each list which should be the working url\n",
    "    links_cleaned = []\n",
    "    for row in links.iterrows():\n",
    "        links_cleaned.append(row[1][0][0])\n",
    "        \n",
    "    # convert to dataframe\n",
    "    links_cleaned = DataFrame(links_cleaned)\n",
    "\n",
    "    return links_cleaned[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "burberry_links = search_google('burberry limited')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use webbrowser library to open all links in browser (if needed)\n",
    "for link in burberry_links:\n",
    "    webbrowser.open(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract key text from company websites "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got the functionality to return links from Google search results, we want to navigate to each link, scrape and format the text to find words with explanatory value after removing stopwords and other standard website text. Let's use the Dyson website as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dyson_links = search_google('Dyson Limited')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inspecting the first search result\n",
    "# Read HTML\n",
    "html = requests.get(dyson_links[0])\n",
    "# Parse HTML into a BeautifulSoup object\n",
    "soup = BeautifulSoup(html.content, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracting text from key html sections\n",
    "[s.extract() for s in soup(['style', 'script', '[document]', 'head', 'title'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all visible text\n",
    "text = soup.getText().encode('ascii','ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to clean up this text by removing HTML tags and new line indicators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_r = str(text).replace('\\\\n','').replace('\\\\t','').replace('\\\\r','')\n",
    "print(text_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stripsymbols(text):\n",
    "    '''\n",
    "    Use regular expressions to do a find-and-replace of HTML text.\n",
    "    Function found online\n",
    "    '''\n",
    "    text = str(text)\n",
    "    x = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text)\n",
    "    x = re.sub('/(^|\\b)@\\S*($|\\b)/',\" \",x)\n",
    "    x = re.sub('/(^|\\b)#\\S*($|\\b)/',\" \",x)\n",
    "    x = re.sub(\"[^a-zA-Z]\",\" \",x)\n",
    "    x = re.sub(r\"(?:\\@|https?\\://)\\S+\", \" \",x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_stripped = stripsymbols(text_r)\n",
    "print(text_stripped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting better! The final step is remove stop words by invoking the Natural Language Toolkit Library we imported earlier. Let's see what kind of stop words are classified in the library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(stopwords.words('English'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a function to remove stopwords from website text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeStopWords(text):\n",
    "    '''\n",
    "    Remove stopwords from given text string\n",
    "    '''\n",
    "    words = [w for w in text if not w in stopwords.words(\"english\")]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare text for function by (a) changing to lowercase (2) remove whitespace from beginning and end \n",
    "# (3) splitting the text to create a list of individual wordsvisible_text_stripped.lower().strip().split() \n",
    "text_stripped_split = text_stripped.lower().strip().split()\n",
    "text_stripped_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, running this through the removeStopWords function gives us something in a much better shape than the original text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_cleaned = removeStopWords(text_stripped_split)\n",
    "text_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapping this all up into a function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleantext(text):\n",
    "    text_r = str(text).replace('\\\\n','').replace('\\\\t','').replace('\\\\r','')\n",
    "    text_stripped = stripsymbols(text_r)\n",
    "    text_stripped_split = text_stripped.lower().strip().split()\n",
    "    text_cleaned = removeStopWords(text_stripped_split)\n",
    "    return ' '.join(text_cleaned) # returns a joined list of the remaining words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dyson_cleaned = cleantext(text) # example of a function\n",
    "dyson_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Natural Language Toolkit to tokenise and tag words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another feature of the Natural Language Toolkit we can exploit is the ability to tag words and categorise them into their 'parts of speech' (i.e. nouns, verbs, adverbs). Perhaps this can be used to tag words from the scraped text, and retrieve nouns under the assumption that they provide the greatest explanatory power.\n",
    "\n",
    "Let's test this with the scraped text from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dyson_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(dyson_cleaned) # tokenize (split) the string\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tags bellow are given according to the Penn Treebank Project here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "The results are quite interesting. Words like 'vacuums', 'cylinders', 'robot' and 'dryers' are correctly given as nouns. However, some words are double-tagged. For instance, 'dyson' is given as a verb as well as a noun. 'Airblade' is given as a past-tense verb. \n",
    "\n",
    "From this example alone, filtering on nouns would yield the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(tokens) # categorise words according to their parts of speech\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep nouns only\n",
    "tagged_nouns = \" \".join([word[0] for word in tagged if word[1] in ['NNS','NN']])\n",
    "tagged_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Difference in list sizes - 1,352 removed words (out of 3,767)\n",
    "len(dyson_cleaned) - len(tagged_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Master function and wordclouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect everything we've built in Section 2 and build our final master functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def key_words(list_of_urls):\n",
    "    '''\n",
    "    From the given list of urls, this function scrapes, cleans (removing symbols and non-nouns)\n",
    "    and gathers all text into a single string\n",
    "    '''\n",
    "    \n",
    "    words_cleaned = '' # Set up empty string\n",
    "    \n",
    "    for website in list_of_urls:\n",
    "        # try / except for troublesome websites\n",
    "        try:\n",
    "            html = requests.get(website)\n",
    "            soup = BeautifulSoup(html.content, 'html5lib')\n",
    "            [s.extract() for s in soup(['style', 'script', '[document]', 'head', 'title'])]\n",
    "            visibleText = soup.getText().encode('ascii','ignore')\n",
    "            words_cleaned += cleantext(visibleText) # Calling the function we built earlier \n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    # Tokenize and tag words\n",
    "    tokens = nltk.word_tokenize(words_cleaned)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Keep nouns only\n",
    "    keywords = \" \".join([word[0] for word in tagged if word[1] in ['NNS','NN']])\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_words(search_google('Dyson Limited')) # testing on an example - takes a few seconds to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is best visualised in a word cloud..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cloud(words):\n",
    "    '''\n",
    "    Function that takes in a string and produces a word cloud\n",
    "    '''\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                          background_color='black',\n",
    "                          width=1800,\n",
    "                          height=1400\n",
    "                         ).generate(words)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cloud(key_words(search_google('Royal Dutch Shell'))) # can take upto a minute to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = cloud(key_words(search_google('carillion')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of scope to improve this. Certain terms from websites that commonly re-occur can be filtered out by updating the stopwords set as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ways to improve this: list of common names, weights and quantities, months and dates,\n",
    "\n",
    "custom_words = ['sun','new','showbiz','tv','uk','john','lewis','partnership','offers','store',\n",
    "'business','company','stores','shop','department','partner','street','london','partners','peter',\n",
    "'jones','duration', 'views', 'minutes', 'month', 'version', 'system','tesco','september','privacy',\n",
    "'policy','customer','service','home', 'company', 'london', 'price', 'offer', 'customer',\n",
    "'service', 'home', 'year', 'london', 'day', 'march', 'business', 'shop','item','level','logo','menu',\n",
    "'account','co','road','centre']\n",
    "\n",
    "\n",
    "#STOPWORDS is a set, so need to use update method\n",
    "STOPWORDS.update(custom_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Google Places API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've relied on Google Search results to hopefully get websites that relate to a company term. However, the Google Places API can be linked to with a company's name and postcode and used to retrieve the company website and other details. \n",
    "\n",
    "Information from Google Places is displayed for certain businesses and places of interest as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/google_places_screenshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin as usual by importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from time import sleep # to pause requests when web-scraping\n",
    "import geopy.distance # to calculate distances\n",
    "import re # regex to remove symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Places API requests can be called with the requests package. By default, you'll be able to send 1,000 requests per day but this can be increased free of charge, up to 150,000 requests per 24 hour period, by enabling billing on the Google API Console to verify your identity. Check quota here (after registering): http://code.google.com/apis/console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "YOUR_API_KEY = '' # enter API key which you'll receive from Google "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check documentation here for info on the various Google Places APIs: https://developers.google.com/places/web-service/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the postcode for London Eye as an example: SE1 7PB. We'll need to input the longitude and latitude for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "postcode = 'SE1 7PB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html = requests.get(\"http://api.postcodes.io/postcodes/\" + postcode)\n",
    "html.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = html.json()['result']['latitude']\n",
    "lon = html.json()['result']['longitude']\n",
    "lat, lon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearby search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Nearby Search lets you search for places within a specified area. You can refine your search request by supplying keywords or specifying the type of place you are searching for. A nearby search request is an HTML URL of the following form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=-33.8670522,151.1957362&radius=500&type=restaurant&keyword=cruise&key=YOUR_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the London Eye as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "london_eye = requests.get('https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=' + \\\n",
    "                          str(lat) + ',' + str(lon)  + '&' + 'keyword=London+Eye' + '&' + \\\n",
    "                          'rankby=distance' + '&' + \\\n",
    "                          'key=' + str(YOUR_API_KEY))\n",
    "london_eye.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the json() file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "london_eye.json()['results'] # list of results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "london_eye.json()['results'][0]['name'] # name of place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le_id = london_eye.json()['results'][0]['place_id'] # id of location \n",
    "le_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got the id, we can get the website from the text search API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before leaving this section, let's build a function that returns the location id from a given set of parameters. We'll need to call upon this function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    '''\n",
    "    Calculate distance (in km) between two sets of latitude and longitude coordinates\n",
    "    Uses geopy package\n",
    "    '''\n",
    "    x = lat1, lon1\n",
    "    y = lat2, lon2\n",
    "    return geopy.distance.vincenty(x, y).km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearby_search(lat,lon,keyword,api_key):\n",
    "    '''\n",
    "    input: latitude and longitude (floats), api_key for Google Places (string)\n",
    "    output: location_id of place closest to co-ordinates and that matches the keyword\n",
    "    if the company is not found, then a message will be given\n",
    "    '''\n",
    "    \n",
    "    # HTML wrapped for arguments\n",
    "    html = requests.get('https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=' + \\\n",
    "                        str(lat) + ',' + str(lon)  + '&' + 'keyword=' + keyword + '&' + \\\n",
    "                        'rankby=distance' + '&' + 'key=' + str(api_key))\n",
    "        \n",
    "    # Error message if location not found\n",
    "    try: \n",
    "        place_id = html.json()['results'][0]['place_id']\n",
    "    except:\n",
    "        return 'Location not found'\n",
    "    \n",
    "    # Check how close the search result is, reject if more than 1km distance\n",
    "    \n",
    "    # Coordinates from Google Places\n",
    "    lat_g = html.json()['results'][0]['geometry']['location']['lat']\n",
    "    lon_g = html.json()['results'][0]['geometry']['location']['lng']\n",
    "\n",
    "    if distance(lat, lon, lat_g, lon_g) > 10: # more than 10km away\n",
    "        return 'Location too far'\n",
    "    \n",
    "    # return html.json()\n",
    "    return place_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing the function\n",
    "nearby_search(51.5028202620979,-0.119252376858172,'London Eye',YOUR_API_KEY) # correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Returns a KFC about 1 mile from the London Eye. Since we can't restrict radius with the rankby parameter, \n",
    "# depending on how strict the business name matching is, we run the risk of mismatches\n",
    "nearby_search(51.5028202620979,-0.119252376858172,'KFC',YOUR_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nearby_search(51.5028202620979,-0.119252376858172,'London',YOUR_API_KEY) # returns 'London' - the entire city?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nearby_search(51.5028202620979,-0.119252376858172,'Eye',YOUR_API_KEY) # Specsavers in the Strand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nearby_search(51.5028202620979,-0.119252376858172,'Wheel',YOUR_API_KEY) # Kwik-fit in Elephant & Castle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nearby_search(51.5028202620979,-0.119252376858172,'Ferris Wheel',YOUR_API_KEY) # Correct - London Eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_search(50.547036619931,-3.49639521511686,'R.B.W. DEVELOPMENTS LTD.',YOUR_API_KEY) # Testing sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QA (reinforce postcode check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the London Eye example, the postcode which is input is 'SE1 7PB', which has corresponding coordinates of (51.5028202620979, -0.119252376858172). Google Nearby Search returns (51.503324, -0.119543) which is close enough.\n",
    "\n",
    "However, if any results are more than a certain distance away (say 1km), we reject them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get coordinates from google places\n",
    "lat_g = london_eye.json()['results'][0]['geometry']['location']['lat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lon_g = london_eye.json()['results'][0]['geometry']['location']['lng']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lat, lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lat_g, lon_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    '''\n",
    "    Calculate distance (in km) between two sets of latitude and longitude coordinates\n",
    "    Uses geopy package\n",
    "    '''\n",
    "    x = lat1, lon1\n",
    "    y = lat2, lon2\n",
    "    return geopy.distance.vincenty(x, y).km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance(lat,lon,lat_g,lon_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Place search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link here: https://developers.google.com/places/web-service/details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a place_id from a Place Search, you can request more details about a particular establishment or point of interest by initiating a Place Details request. A Place Details request returns more comprehensive information about the indicated place such as its complete address, phone number, user rating and reviews. A Place Details request is an HTTP URL of the following form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://maps.googleapis.com/maps/api/place/details/output?parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling this on the London Eye place id.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "london_eye_website = requests.get('https://maps.googleapis.com/maps/api/place/details/json?' + \\\n",
    "                    'placeid=' + le_id + '&' + 'key=' + str(YOUR_API_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "london_eye_website.json()['result'] # list of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "london_eye_website.json()['result']['formatted_address']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "london_eye_website.json()['result']['international_phone_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "london_eye_website.json()['result']['opening_hours'] # dictionary of opening hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "london_eye_website.json()['result']['reviews'] # reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "london_eye_website.json()['result']['reviews'] # example of a review - NLP potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "london_eye_website.json()['result']['website'] # place website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let's build a helper function for convenience in the following section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def place_search(place_id):\n",
    "    '''\n",
    "    input: place_id from Nearby Search API (string)\n",
    "    output: company website (if found)\n",
    "    If a website is not found, then an error message will be given\n",
    "    '''\n",
    "    \n",
    "    # exception in case place_id doesn't yield results in Place Search\n",
    "    try:\n",
    "        html = requests.get('https://maps.googleapis.com/maps/api/place/details/json?' + \\\n",
    "                    'placeid=' + place_id + '&' + 'key=' + str(YOUR_API_KEY))\n",
    "    except:\n",
    "        return 'No results in Place Search'\n",
    "    \n",
    "    # exception if website is not found\n",
    "    try:\n",
    "        website = html.json()['result']['website']\n",
    "    except:\n",
    "        return 'Website not found'\n",
    "    \n",
    "    return website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing function with London Eye ID\n",
    "place_search('ChIJc2nSALkEdkgRkuoJJBfzkUI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing function with other IDs\n",
    "place_search('ChIJmbKqQlgDdkgRfkx7-tWdNU4') # KFC in Lambeth - not branch-specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_search('ChIJdd4hrwug2EcRmSrV3Vo6llI') # for London - website not avaiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_search('ChIJDYZYNsoEdkgRHCQ30Uysy_M') # Specsavers - branch-specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "place_search('ChIJWwBL554EdkgRluPLRtvCKek') # Kwik-fit - branch specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to return website from given companies house details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's bring some of the pieces in the previous sections together to build a function that returns the company website (if found) from details found in the publicly-available Companies House dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Section 1.4, we exported a cleaned-up dataset of Companies House to a local directly. Let's import this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = pd.read_csv('ch_2018-02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be running the business name and the postcode through the Google Places API. The business name should be given but we'll need to exclude those records where the postcode is not given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_pc = ch[ch.postcode.notnull()].copy()\n",
    "ch_pc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we've got our dataset. Let's now put our function together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_website(company_name,postcode):\n",
    "    '''\n",
    "    Input: company name and postcode (both strings)\n",
    "    Output: Company website\n",
    "    '''\n",
    "    \n",
    "    # Step 1: Convert postcodes to latitude and longitude using postcodes.io\n",
    "    # Some companies may give invalid postcodes in Companies House so build exception\n",
    "    \n",
    "    try: \n",
    "        html = requests.get(\"http://api.postcodes.io/postcodes/\" + postcode)\n",
    "        lat = html.json()['result']['latitude']\n",
    "        lon = html.json()['result']['longitude']\n",
    "    except:\n",
    "        return 'Postcode not found'\n",
    "    \n",
    "    # Step 2: Get place_id by calling upon a Nearby Search (see Section 3.2)\n",
    "    place_id = nearby_search(lat,lon,company_name,YOUR_API_KEY)\n",
    "    \n",
    "    # Step 3: Return website by feeding place_id into Place Search\n",
    "    website = place_search(place_id)\n",
    "    \n",
    "    return website    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing function on Companies House"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the fun stuff: let's test this function on a sample of companies in Companies House and see how many companies we get websites for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random sample of a 100 companies from those entries with a postcode, only return name and postcode\n",
    "ch_random_100 = ch_pc.sample(100)[['name','postcode']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_random_100.loc['website'] = np.NAN # set up blank column\n",
    "ch_random_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve websites for each company with a for loop\n",
    "i = 1\n",
    "for index, row in ch_random_100.iterrows():\n",
    "    ch_random_100.loc[index,'website'] = return_website(row[0],row[1])\n",
    "    print(str(i) + ' - ' + str(row[0]) + ' - ' + str(row[1]) + ' - ' + str(row[2]))\n",
    "    i += 1\n",
    "    sleep(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ch_random_100['website'].value_counts().head() # 17 websites out of 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_random_100.to_csv('sample_of_100_websites.csv') # save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can improve the match-rate by removing 'LTD. and 'LIMITED' from the end of the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_random_100.loc[:,'website_2'] = np.NAN # set up blank column\n",
    "ch_random_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_random_100 = pd.read_csv('sample_of_100_websites.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ch_random_100 = ch_random_100.head(100).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardise(company_name):\n",
    "    stopwords = ['limited','ltd','ltd.','lp']\n",
    "    querywords = str(company_name).split()\n",
    "    resultwords  = [word for word in querywords if word.lower() not in stopwords]\n",
    "    result = ' '.join(resultwords)\n",
    "    result_no_symbols = re.sub(r'[^\\w]', '', result) # removes symbols\n",
    "    return result_no_symbols.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardise('SHARP & SHINE INDUSTRIAL CO., LTD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_random_100['name_stripped'] = ch_random_100['name'].apply(standardise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_random_100 = pd.read_csv('datasets/sample_of_100_websites_2.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_random_100.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in ch_random_100.iterrows():\n",
    "    print(row[0], row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve websites for each company with a for loop\n",
    "i = 1\n",
    "for index, row in ch_random_100.iterrows():\n",
    "    ch_random_100.loc[index,'website_4'] = return_website(row[0],row[1])\n",
    "    print(str(i) + ' - ' + str(row[0]) + ' - ' + str(row[1]) + ' - ' + str(row[6]))\n",
    "    i += 1\n",
    "    sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_random_100['website_4'].value_counts() # 18 websites out of 100, lose some cases, gain others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_random_100.to_csv('sample_of_100_websites_3.csv') # save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on NYER LEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import postcode lookups for the North Yorkshire and East Riding LEP from the National Postcode Lookup Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyer_postcodes = pd.read_csv('nyer_postcode.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyer_postcodes.head() # three postcodes to lookup for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do an inner join from the CH data to the NYER postcodes on the POSTCODE_PCDS column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ch_nyer = pd.merge(ch_pc, nyer_postcodes, how='inner', left_on='postcode', right_on='POSTCODE_PCDS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_nyer.shape # 12,486 companies. Similiar to what we're seeing in CSAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_nyer.loc[:,'website'] = np.NAN # set up blank column\n",
    "ch_nyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_nyer_1000 = ch_nyer.loc[:999,].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_nyer_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve websites for each company with a for loop\n",
    "for index, row in ch_nyer_1000.iterrows():\n",
    "    if pd.notnull(row[28]) == True:\n",
    "        continue\n",
    "    ch_nyer_1000.loc[index,'website'] = return_website(row[0],row[7]) \n",
    "    print(str(index) + ' - ' + str(row[0]) + ' - ' + str(row[7]) + ' - ' + str(ch_nyer_1000.loc[index,'website']))\n",
    "    sleep(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_nyer_1000['website'].value_counts() # 3,176 websites found for 12,486 (25%), some of these are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ch_nyer_1000.to_csv('nyer_with_websites.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping social media accounts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the websites from Google Places, we can scrape social media accounts and other interesting sites (i.e. trip advisor) for these and then subsequently scrape activity, and follower information from those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import our NYER dataset from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyer = pd.read_csv('nyer_with_websites.csv', index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyer_websites = nyer[nyer['website'] != 'Website not found'] # select those with websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyer_websites.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing 'AUTO 66 RACING LIMITED', with crn 00980520 for our test as this has social media handles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auto = nyer_websites[nyer_websites['crn'] == '00980520']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(auto.website.values) # website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python package exists to extract social media: https://pypi.python.org/pypi/extract-social-media/0.4.0. Can also used regex: https://github.com/lorey/social-media-profiles-regexs. We'll test the package for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code copied and pasted from above link\n",
    "import requests\n",
    "from html_to_etree import parse_html_bytes\n",
    "from extract_social_media import find_links_tree # install first with setup.py from link above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = requests.get('http://www.oliversmountracing.com/')\n",
    "tree = parse_html_bytes(res.content, res.headers.get('content-type'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "set(find_links_tree(tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapping this up in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_social_media(url):\n",
    "    '''\n",
    "    Retrieve social media accounts from a company website\n",
    "    Input: string of URL\n",
    "    Output: dictionary of links\n",
    "    '''\n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "        tree = parse_html_bytes(res.content, res.headers.get('content-type'))\n",
    "        return set(find_links_tree(tree))\n",
    "    except:\n",
    "        return 'Not found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_social_media('http://www.oliversmountracing.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyer_websites.website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this on dataset (took about an hour to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyer['social_media'] = nyer.website.apply(get_social_media)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting twitter followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyer_websites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyer.social_media.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyer_sm = nyer[(nyer['social_media'] != 'Not found') & (nyer['social_media'] != {})] # select those with websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyer_sm.social_media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyer_sm.social_media.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyer.to_csv('nyer_with_websites_sm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "livereveal": {
   "backimage": "slide-background.jpg",
   "scroll": true,
   "theme": "serif",
   "transition": "zoom"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "467px",
    "left": "1094px",
    "top": "270.5px",
    "width": "324px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
